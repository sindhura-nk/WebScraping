# -*- coding: utf-8 -*-
"""Webscraping Article title & text.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Dvw5Uc-lyx4Y5DWcYrqTTtZh0rAEpiOe
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install beautifulsoup4 requests

import requests
import pandas as pd

links = pd.read_excel('Input.xlsx')
links.head()

links['URL']

from bs4 import BeautifulSoup

article_texts = []

for id,i in zip(links['URL_ID'],links['URL']):
  response = requests.get(i)

  if response.status_code == 200:
    soup = BeautifulSoup(response.content)
    title = soup.select('h1.entry-title,h1.tdb-title-text')[0].text

    text1 = soup.find_all('div',class_='td-post-content tagdiv-type')
    text2 = soup.find_all('div',class_='div.tdb-block-inner td-fix-index')

    if text1:
      texts = text1[0].text.strip('\n')

    elif text2:
      texts = text2[0].text.strip('\n')

    #print(i,'\n',title,'\n',texts)
    title_text = f"{title}..{texts}"
    article_texts.append(title_text)
  else:
    title_text = 'Webpage not found'
    #print(i,'\n',title,'\n',texts)
    #article_texts.append(texts)

  # save the article text to file with filename as URL ID
  file_name = f"{id}.txt"

  with open(file_name,'w',encoding='utf-8') as file:
    file.write(title_text)

  #print(f"Article for URL ID '{id}' saved to {file_name}") # for my reference

"""## Text Analysis, Sentiment Analysis"""

#pip install nltk

import nltk

nltk.download('all')

"""## Text Preprocessing

#### 1.1	Cleaning using Stop Words Lists
First each text is tokenized into words and then from there the stopwords are removed by iterating through tokenized words
"""

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize,sent_tokenize

'''
Adding all the filenames to 'filename' list. creating a set 'stopwords
Opening files one by one and adding the stopwords to 'stopwords' set
'''
filenames = ['StopWords\\StopWords_Auditor.txt','StopWords\\StopWords_Currencies.txt',
             'StopWords\\StopWords_DatesandNumbers.txt','StopWords\\StopWords_Generic.txt',
             'StopWords\\StopWords_GenericLong.txt','StopWords\\StopWords_Geographic.txt',
             'StopWords\\StopWords_Names.txt']

stopwords = set()

for i in filenames:
  with open(i,'r',encoding='latin-1') as file:
    for stop in file:
      stopwords.add(stop.strip().lower())
#print(stopwords) # for my reference

'''
creating a function that will remove the stopwords from the given text
'''
def remove_stopwords(text,stopword):
  words = word_tokenize(text)
  cleaned_words = []
  for word in words:
    if word not in stopword:
      cleaned_words.append(word)
  cleaned_text = ' '.join(cleaned_words)
  return cleaned_text

'''
Opening all the article text files to split the text words into tokens. And then remove the stopwords from them.
'''
for id in links['URL_ID']:
  filename1 = f"{id}.txt"
  with open(filename1,'r',encoding='utf-8') as file:
    cleaned_text = remove_stopwords(file.read(),stopwords)

    filename2 = f"cleaned {id}.txt"
    with open(filename2,'w',encoding='utf-8') as file2:
      file2.write(cleaned_text)

"""#### 1.2	Creating a dictionary of Positive and Negative words"""

'''
Reading postive and negative words
'''
pos_set = open('MasterDictionary\\positive-words.txt',encoding='latin-1').read()
neg_set = open('MasterDictionary\\negative-words.txt',encoding='latin-1').read()

positive_list=pos_set.split('\n')
negative_list=neg_set.split('\n')

#view the first elements in the lists and the length of the lists
print(positive_list[:10],'\n',len(positive_list))
print(negative_list[:10],'\n',len(negative_list))

dct = {'positive_words':set(),'negative_words':set()} # Using set to record every positive/negative word only once.
for id in links['URL_ID']:
  filename = f"cleaned {id}.txt"
  with open(filename,'r',encoding='utf-8') as file:
    x=file.read()
    p_words = [word for word in x.split() if word in positive_list]
    n_words = [word for word in x.split() if word in negative_list]
    dct['positive_words'].update(p_words)
    dct['negative_words'].update(n_words)
print(dct)

"""#### Calculating Positive score, Negative score, Polarity score"""

text = 'Estimating impact COVID-19 world work .. COVID-19 unprecedented pandemic “ Can-be ” possibility great leaders action rising 2020 . The corollary pandemic prodigious analysis UN Department Economic Social Affairs ( DESA ) stated COVID-19 pandemic disrupting global supply chains international trade turn shrink global economy 1 percent 2020 , reversal previous forecast 2.5 percent growth . More 6.6 Americans filed unemployment claims economic downturn expected worst recession Great Depression stated IMF . India facing biggest crisis decades , three-week lockdown initially , extending 1.3 people result economic recession , millions job losses starvation poor . It economic contagion spreading disease . On contrary nature pressed reset , environment noticeable benefit scenario . Water clearer , air turning breathable news pictures easily newspapers , telly , social media platforms basis . But storyline present , future ? Are predictions made financial institutions thought leaders ? Or unexpected happen make human capabilities ? After COVID-19 , surprise digitally business provide digital infrastructure customer experience advancement technology lacking present scenario . The touch screens find vast applications places including hotels , hostels , shopping malls . All processes interdependent agile turn resulting productivity goods , systematic backup giving experience consumer hard situations . The focus making contactless systems Artificial intelligence machine learning rapid in-demand . Not , medical infrastructure services boost developing systems handle situation afterward . The human machine interaction increase facilitating production , delivery surveillance inculcating . In area COVID leave retail sector banking sector making modify fully change architectures . Even existing systems gifted modifications , ensuring work hard environment human intervention . The Companies rethink policies shaping manage resources ensuring safety , agility , work top priority . The work home idea provide base bigger companies ensuring develop infrastructure resources giving birth concept work culture . Various existing industries related delivery travel adopt idea collaboration industries result options consumers routine Apps . New businesses existing ventures resulting competition business world turning brownie point consumer experience bandwidth choices . Start-up culture expected stupendous growth providing legitimate opportunity feel digital world . The innovations hear jargons related tech upcoming years . Since global economy fumbled , countries turn vamoose phase . They make connections nurturing give relationship business relationships unexpected move making position countries sliding indexes . Even rival countries join hands hard time dismissed totally predator countries . It surprising scenarios handled emperor minds opponents agenda elections . Unfortunately , nature start face ill effects chart development economy exponential growth . It unfortunate inverse relationship economic growth environmental destruction exists carbon emission increases economic development . As shown graph recession carbon emission decreases harmful gases , turning gift nature doom financial markets . But possibility start-ups emerge idea environment conservation collaborating NGOs innovating capability reduce ill effect present origin point . In culmination pandemic “ Blessing disguise ” producing massive impact macro micro-economy . The virus brought unexpected results uncovered loopholes unlatched opportunities growth , forcing define spheres . Quite ago chit-chatting digital Era , benefiting , COVID-19 revealed making realize beginning digital Era lot needed happen future . It obvious Indian economy global economy facing face aftereffects similar virus , economy rise deep grounds candlestick individual collaboration ! The businesses rise , people jobs , vendors start profits fall places megalithic competitive environment rise till decades nature shows ’ color human abide outcomes , learning lessons keeping cycle fall growth running . Blackcoffer Insights 17 : Vedansh Dubey , TCS Indore M.P'
words = word_tokenize(text)
print(words)

pos_count = 0
neg_count = 0

for word in words:
  if word in dct['positive_words']:
    pos_count = pos_count + 1
  elif word in dct['negative_words']:
    neg_count = neg_count + 1
print(f"Positive Score: {pos_count}\nNegative Score: {neg_count}")

'''
Capturing all the required details into a dictionary
'''
derived_variables = {'URL_ID':[],'URL':[],'POSITIVE SCORE':[],'NEGATIVE SCORE':[],'POLARITY SCORE':[],'SUBJECTIVITY SCORE':[]}
for id,link in zip(links['URL_ID'],links['URL']):
  filename = f"cleaned {id}.txt"
  with open(filename,'r',encoding='utf-8') as file:
    words = word_tokenize(file.read())

    pos_count = 0
    neg_count = 0

    for word in words:
      if word in dct['positive_words']:
        pos_count = pos_count + 1
      elif word in dct['negative_words']:
        neg_count = neg_count + 1

    polarity_score = (pos_count-neg_count)/((pos_count+neg_count)+0.000001)
    subjectivity_score = (pos_count+neg_count)/(len(words)+0.000001)

    derived_variables['URL_ID'].append(id)
    derived_variables['URL'].append(link)
    derived_variables['POSITIVE SCORE'].append(pos_count)
    derived_variables['NEGATIVE SCORE'].append(neg_count)
    derived_variables['POLARITY SCORE'].append(round(polarity_score,2))
    derived_variables['SUBJECTIVITY SCORE'].append(round(subjectivity_score,2))

print(derived_variables) # for my reference

"""#### 2	Analysis of Readability"""

#pip install nltk pyphen

import re # regex to remove punchuation from text
import pyphen # pyphen for syllable count

'''
Function to calculate syllable per word
'''
# let's create a english dictionary using pyphen to calcualte number of syllables
dic = pyphen.Pyphen(lang='en')
def count_syllable(word):
  if word.endswith('es') or word.endswith('ed'):
    return dic.inserted(word).count('-')
  else:
    return len(dic.inserted(word).split('-'))

'''
creating lists for new keys in the existing dictionary
'''
derived_variables['AVG SENTENCE LENGTH'] = []
derived_variables['PERCENTAGE OF COMPLEX WORDS'] = []
derived_variables['FOG INDEX'] = []
derived_variables['AVG NUMBER OF WORDS PER SENTENCE'] = []
derived_variables['COMPLEX WORD COUNT'] = []
derived_variables['WORD COUNT'] = []
derived_variables['SYLLABLE PER WORD'] = []
derived_variables['PERSONAL PRONOUNS'] = []
derived_variables['AVG WORD LENGTH'] = []

for id in links['URL_ID']:
  filename = f"cleaned {id}.txt"
  with open(filename,'r',encoding='utf-8') as file:
    x = file.read()
    sentences = sent_tokenize(x)

    pattern = r'[^\w\s]'
    new_x = re.sub(pattern,'',x)
    words = word_tokenize(new_x)

    # as per doc, number of words are the ones without the stop words and punctuations
    number_of_words = len(words)
    number_of_sentences = len(sentences)

    # calculating syllables in a word, characters in a word and complex word count
    number_of_complex_words = 0
    characters = 0
    for word in words:
      characters += len(word) # calculating numbers of characters per word
      syllables = count_syllable(word) # calculating number of syllables per word
      if syllables > 2: # as per doc, Complex words are words in the text that contain more than two syllables.
        number_of_complex_words += 1

    # calculating personal pronouns using regex
    pattern2 = r'\b(?:I|we|my|ours|us)\b'
    regex = re.compile(pattern2,flags=re.IGNORECASE)
    pronouns = []
    for word in words:
      matches = regex.findall(word)
      pronouns.extend(matches)

    Average_Sentence_Length = (number_of_words)/(number_of_sentences)
    percentage_complex_words = (number_of_complex_words)/(number_of_words)
    if percentage_complex_words != 0:
      Fog_Index = 0.4 * (Average_Sentence_Length/percentage_complex_words)
    else:
      Fog_Index = 'NA'
    personal_pronouns = len(pronouns)
    average_word_length = (characters)/(number_of_words)

    # Saving all the above derived values to 'derived_variables' dictionary
    derived_variables['AVG SENTENCE LENGTH'].append(Average_Sentence_Length)
    derived_variables['PERCENTAGE OF COMPLEX WORDS'].append(percentage_complex_words)
    derived_variables['FOG INDEX'].append(Fog_Index)
    derived_variables['AVG NUMBER OF WORDS PER SENTENCE'].append(Average_Sentence_Length)
    derived_variables['COMPLEX WORD COUNT'].append(number_of_complex_words)
    derived_variables['WORD COUNT'].append(number_of_words)
    derived_variables['SYLLABLE PER WORD'].append(syllables)
    derived_variables['PERSONAL PRONOUNS'].append(personal_pronouns)
    derived_variables['AVG WORD LENGTH'].append(average_word_length)

'''
converting derived_variables dictionary to dataframe
'''
df = pd.DataFrame(derived_variables)
df.head()

df.to_excel('Output Data Structure.xlsx',index=False)